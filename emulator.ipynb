{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperModel\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_search(HyperModel):\n",
    "\n",
    "  def __init__(self, x_shape, y_shape):\n",
    "    self.x_shape = x_shape\n",
    "    self.y_shape = y_shape\n",
    "\n",
    "\n",
    "  def build(self, hp):\n",
    "    model = keras.Sequential()\n",
    "    nx, ny = self.x_shape, self.y_shape\n",
    "    for i in range(hp.Int('n_layers', min_value=1, max_value=6)):\n",
    "      model.add(\n",
    "        layers.Dense(\n",
    "          units=hp.Int(f\"units_{i}\", min_value=nx, max_value=10*nx, step=nx),\n",
    "          activation='relu'\n",
    "        )\n",
    "      )\n",
    "    model.add(layers.Dense(ny, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class neural_network:\n",
    "\n",
    "  def __init__(self, filename, x_list, y_list, project):\n",
    "    self.filename = filename\n",
    "    self.x_list = x_list\n",
    "    self.y_list = y_list\n",
    "    self.project = project\n",
    "    self.scaler1 = MinMaxScaler\n",
    "    self.scaler2 = MinMaxScaler\n",
    "  \n",
    "\n",
    "  def read_data(self):\n",
    "    file = h5py.File(self.filename, 'r')\n",
    "    mchain = file['markov_chain_0']\n",
    "    nlines = mchain['nlines'][0]\n",
    "    dtable = mchain['data']\n",
    "    x = np.zeros((nlines, len(self.x_list)))\n",
    "    y = np.zeros((nlines, len(self.y_list)))\n",
    "    for i, params in enumerate(self.x_list):\n",
    "      x[:,i] = np.array(dtable[self.x_list[i]])\n",
    "    for i, quants in enumerate(self.y_list):\n",
    "      y[:,i] = np.array(dtable[self.y_list[i]])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "  def scale_data(self, x, y, is_normed=False, x_raw=[], y_raw=[]):\n",
    "    xy = np.concatenate((x,y), axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    if (not is_normed):\n",
    "      xy_scaled = scaler.fit_transform(xy)\n",
    "    else:\n",
    "      xy_raw = np.concatenate((x_raw,y_raw), axis=1)\n",
    "      scaler.fit(xy_raw)\n",
    "      xy_scaled = scaler.inverse_transform(xy)\n",
    "    x_scaled = xy_scaled[:,:x.shape[1]]\n",
    "    y_scaled = xy_scaled[:,x.shape[1]:]\n",
    "    return x_scaled, y_scaled\n",
    "    \n",
    "\n",
    "  def process_data(self):\n",
    "    x, y = self.read_data()\n",
    "    xs, ys = self.scale_data(x, y)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "  def build_model(self, arch, act_func, loss, optimizer):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(arch[1], input_shape=(arch[0],), \\\n",
    "                           activation=act_func[0]))\n",
    "    for i, nn in enumerate(arch):\n",
    "      if (i>1 and i<len(arch)-1):\n",
    "        model.add(layers.Dense(arch[i], activation=act_func[0]))\n",
    "    model.add(layers.Dense(arch[len(arch)-1], activation=act_func[1]))\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "  def load_model(self, is_tuned=False):\n",
    "    if (not is_tuned):\n",
    "      model_path = 'out/checkpoints/'\n",
    "      if not os.path.exists(model_path):\n",
    "        print(\"Loading failed: Trained model not found!\")\n",
    "        exit(1)\n",
    "    else:\n",
    "      model_path = 'out/model_'+self.project+'.h5'\n",
    "      if not os.path.exists(model_path):\n",
    "        print(\"Loading failed: Tuned model not found!\")\n",
    "        exit(1)\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "  def set_callbacks(self, model_ckpt=False, backup_restore=False, \\\n",
    "                    early_stop=False, var_lr=False, reduce_lr=False, \\\n",
    "                      term_nan=False, verb=0):\n",
    "    call_backs = []\n",
    "    \n",
    "    if (model_ckpt):\n",
    "      ckpt_path = 'out/checkpoints/'\n",
    "      if not os.path.exists(ckpt_path):\n",
    "        os.makedirs(ckpt_path)\n",
    "      save_wgts = callbacks.ModelCheckpoint(filepath=ckpt_path, verbose=verb, \\\n",
    "                                            save_best_only=True)\n",
    "      call_backs.append(save_wgts)\n",
    "\n",
    "    if (backup_restore):\n",
    "      backup_dir='out/backup/'\n",
    "      if not os.path.exists(backup_dir):\n",
    "        os.makedirs(backup_dir)\n",
    "      backup_wgts = callbacks.BackupAndRestore(backup_dir, save_freq='epoch', \\\n",
    "                                           delete_checkpoint=True)\n",
    "      call_backs.append(backup_wgts)\n",
    "\n",
    "    if (early_stop):\n",
    "      stop_early = callbacks.EarlyStopping(monitor='loss', min_delta=1.0e-6, \\\n",
    "                                           patience=5, verbose=verb)\n",
    "      call_backs.append(stop_early)\n",
    "\n",
    "    if (term_nan):\n",
    "      term_on_nan = callbacks.TerminateOnNaN()\n",
    "      call_backs.append(term_on_nan)\n",
    "\n",
    "    return call_backs\n",
    "  \n",
    "\n",
    "  def train(self):\n",
    "    x, y = self.process_data()\n",
    "    x_tr, x_tv, y_tr, y_tv = \\\n",
    "      train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "    x_ts, x_vl, y_ts, y_vl = \\\n",
    "      train_test_split(x_tv, y_tv, test_size=0.25, shuffle=True, random_state=42)\n",
    "    call_backs = self.set_callbacks(model_ckpt=True, early_stop=True, \\\n",
    "                                   term_nan=True, verb=0)\n",
    "    model = self.load_model(is_tuned=True)\n",
    "    train = model.fit(x=x_tr, y=y_tr, batch_size=512, validation_data=(x_ts,y_ts), \\\n",
    "                    epochs=5000, callbacks=call_backs, verbose=2)\n",
    "    loss = model.evaluate(x_vl, y_vl, verbose=0)\n",
    "    print(\"Loss = {:.4e}\".format(loss))\n",
    "    return train\n",
    "  \n",
    "\n",
    "  def predict(self, x):\n",
    "    model_path = 'out/checkpoints/'\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    y = model(x)\n",
    "    return y\n",
    "  \n",
    "\n",
    "  def end_session(self):\n",
    "    from numba import cuda\n",
    "    print('Releasing VRAM...')\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    print('GPU session ended.')\n",
    "\n",
    "\n",
    "  def emulate(self):\n",
    "    pass\n",
    "\n",
    "\n",
    "  def set_tuner(self, hypermodel):\n",
    "    tuner = kt.GridSearch(\n",
    "      hypermodel, \n",
    "      objective='val_loss', \n",
    "      max_trials=1000, \n",
    "      seed=42,\n",
    "      hyperparameters=None,\n",
    "      tune_new_entries=True,\n",
    "      allow_new_entries=True,\n",
    "      max_retries_per_trial=0,\n",
    "      max_consecutive_failed_trials=3,\n",
    "      executions_per_trial=1, \n",
    "      directory='out/grid_search/',\n",
    "      project_name=self.project, \n",
    "      overwrite=True\n",
    "    )\n",
    "    tuner.search_space_summary()\n",
    "    return tuner\n",
    "\n",
    "\n",
    "  def search(self):\n",
    "    x, y = self.process_data()\n",
    "    x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size=0.2, \\\n",
    "                                              shuffle=True, random_state=42)\n",
    "    x_shape, y_shape = len(self.x_list), len(self.y_list)\n",
    "    hypermodel = hyper_search(x_shape, y_shape)\n",
    "    tuner = self.set_tuner(hypermodel)\n",
    "    call_backs = self.set_callbacks(early_stop=True)\n",
    "    tuner.search(x_tr, y_tr, batch_size=512, epochs=1000, validation_data=(x_ts, y_ts), \\\n",
    "                callbacks=call_backs, verbose=2)\n",
    "    tuner.results_summary(num_trials=5)\n",
    "    #best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    #best_model = hypermodel.build(best_hp)\n",
    "    models = tuner.get_best_models(num_models=1)\n",
    "    best_model = models[0]\n",
    "    best_model.build(input_shape=(None, x_shape))\n",
    "    best_model.summary()\n",
    "    best_model.save('out/model_'+self.project+'.h5')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='out/nl_20'\n",
    "x_list=['a','alpha','param_S','param_L','csq1','trans1','csq2','trans2','csq3', \\\n",
    "        'M_chirp_det','q','z_cdf','m1_gw19','mf_6304','mf_6397','mf_M13','mf_M28', \\\n",
    "        'mf_M30','mf_wCen','mf_X7','mf_1810b','mf_1724b','mf_1702','mf_0030', \\\n",
    "        'mf_0740','mean_NS','log10_width_NS','skewness_NS','mean_WD','log10_width_WD', \\\n",
    "        'skewness_WD','mean_LMS','log10_width_LMS','skewness_LMS','M_J0453p','M_J0453c', \\\n",
    "        'M_J1906p','M_J1906c','M_B1534p','M_B1534c','M_B1913p','M_B1913c','M_B2127p', \\\n",
    "        'M_B2127c','M_J0737A','M_J0737B','M_J1756p','M_J1756c','M_J1807p','M_J1807c', \\\n",
    "        'M_J1518p','M_J1518c','M_J1811p','M_J1811c','M_J1829p','M_J1829c','M_J2045', \\\n",
    "        'M_J2053','M_J1713','M_B1855','M_J0751','M_J1141','M_J1738','M_J1614','M_J0348', \\\n",
    "        'M_J2222','M_J2234','M_J1949','M_J1012','M_J0437','M_J1909','M_J1802','M_J1911', \\\n",
    "        'M_J2043','M_J0337','M_J1946','M_J1918','M_J1600','M_J0621','M_B2303','M_J0024', \\\n",
    "        'M_J0514','M_B1516','M_J1748I','M_J1748J','M_B1802','M_B1911','M_J0740','M_CygX2', \\\n",
    "        'M_XTEJ2123','M_4U1822','M_HerX1','M_2S0921']\n",
    "y_list=['log_wgt']\n",
    "project = 'nl'\n",
    "em = neural_network(file_name, x_list, y_list, project)\n",
    "em.search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = em.load_model(is_tuned=True)\n",
    "x, y = em.process_data()\n",
    "yp = model(x)\n",
    "print(np.average(yp-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.semilogy(train.history['loss'], ls='--', color='red', label='train')\n",
    "plt.semilogy(train.history['val_loss'], color='orange', label='test')\n",
    "plt.grid()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Training History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=h5py.File('out/ml_19','r')\n",
    "markov_chain=file['markov_chain_0']\n",
    "list(markov_chain.keys())\n",
    "dtable = markov_chain['data']\n",
    "list(dtable.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lines=markov_chain['nlines'][0]\n",
    "n_lines.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_0740=np.array(col_names['mf_0740'])\n",
    "mf_0740.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
